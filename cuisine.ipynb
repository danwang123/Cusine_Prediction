{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataset\n",
    "import pandas as pd\n",
    "df=pd.read_json(r'E:\\what is cooking\\train.json')\n",
    "df2 = pd.DataFrame([x for x in df['ingredients'].apply(lambda item: dict(map(lambda x: (x,1),item))).values]).fillna(0)\n",
    "data=df2.join(df[['cuisine','id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\canopy\\User\\lib\\site-packages\\gensim-0.12.3-py2.7-win-amd64.egg\\gensim\\models\\word2vec.py:675: UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\"C extension not loaded for Word2Vec, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "num_features = 600    # Word vector dimensionality\n",
    "min_word_count = 1   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "model = Word2Vec(df['ingredients'].values, workers=num_workers, size=num_features, min_count = min_word_count,\n",
    "                 window = context, sample = downsampling, seed=1)\n",
    "\n",
    "# If you don't plan to train the model any further, calling\n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "            nwords+=1\n",
    "    if nwords:\n",
    "        featureVec = np.divide(featureVec,nwords)\n",
    "        return featureVec\n",
    "    else:\n",
    "        return np.zeros((num_features,),dtype=\"float32\")\n",
    "def getAvgFeatureVecs(ingredients, model, num_features):\n",
    "    ingredientFeatureVecs = np.zeros((len(ingredients),num_features),dtype=\"float32\")\n",
    "    for index,ingredient in enumerate(ingredients):\n",
    "        ingredientFeatureVecs[index] = makeFeatureVec(ingredient, model, num_features)\n",
    "    return ingredientFeatureVecs\n",
    "\n",
    "\n",
    "X_word2vec=getAvgFeatureVecs(df['ingredients'].values, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the ingredient to index pair\n",
    "ingredients=[]\n",
    "for index, ingredient in enumerate(data.columns.values):\n",
    "    ingredients.append((index,ingredient))\n",
    "ingredients=dict(ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating data labels, denoted by Y\n",
    "d=[]\n",
    "cuisines=data['cuisine'].unique()\n",
    "for index,value in enumerate(cuisines):\n",
    "    d.append((value,index))\n",
    "d=dict(d)\n",
    "label=data['cuisine'].apply(lambda x: d[x])\n",
    "Y=label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the feature vectors, denoted by X\n",
    "from scipy import sparse\n",
    "X=sparse.csr_matrix(data.iloc[:,0:6714].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving memory\n",
    "del data,df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training and testing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "# X_word2vec_train, X_word2vec_test, Y_word2vec_train, Y_word2vec_test = train_test_split(X_word2vec, Y, \n",
    "#                                                                                         test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building covariance matrix for each restraurants\n",
    "from sklearn.covariance import empirical_covariance \n",
    "combinations=[]\n",
    "for cuisine in range(0,20):\n",
    "    #print 'Cuisine:',cuisine,'\\n'    \n",
    "    chinese=[]\n",
    "    for i in range(0,39774):\n",
    "        if Y[i]==cuisine:\n",
    "            chinese.append(X[i])\n",
    "    C=empirical_covariance(chinese)\n",
    "    one=np.amax(C)\n",
    "    C=C/one\n",
    "#Merge top combinations into one matrix\n",
    "    for i in range(0,6714):\n",
    "        for j in range(i+1,6714):\n",
    "            if (C[i][j]>0.05):                \n",
    "                #print 'index is [',i,',',j,']\\n',ingredients[i],',',ingredients[j],'\\n'\n",
    "                combinations.append([i,j])\n",
    "            \n",
    "    #print '********************************************************\\n'\n",
    "#Get rid of the repeated values\n",
    "combinations= [combinations[i] for i in range(len(combinations)) if combinations[i] not in combinations[:i]]\n",
    "#Adding combinations for original matrix\n",
    "combinedTrain=[]\n",
    "for j in range(0,39774):    \n",
    "    combinedTrain.append(X[j])\n",
    "    num=len(combinations)\n",
    "    for i in range(0,num):\n",
    "        if (X[j][combinations[i][0]]==1) and (X[j][combinations[i][1]]==1) :\n",
    "            combinedTrain[j]=np.append(combinedTrain[j],[1])\n",
    "        else:\n",
    "            combinedTrain[j]=np.append(combinedTrain[j],[0])\n",
    "Ctrain=combinedTrain[0:30000]\n",
    "Ctest=combinedTrain[30000:39774]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "pca=PCA()\n",
    "pca.fit(X)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.axes([.2, .2, .7, .7])\n",
    "plt.plot(pca.explained_variance_, linewidth=2)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_variance_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fitting bag of word model into different estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn import svm, grid_search\n",
    "parameters = {'criterion':('gini', 'entropy'), 'C':[1e1,1e2,1e3,1e4]}\n",
    "svr = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(svr, parameters)\n",
    "clf.fit(X_train,Y_train) \n",
    "print clf.grid_scores_ , ' \\n ','Best Estimator',clf.best_estimator_\n",
    "\n",
    "#best svm\n",
    "best_svr = svm.SVC(C=1000)\n",
    "best_svr.fit(X_train,Y_train) \n",
    "sum(svr.predict(X_test)==Y_test) # 0.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm, grid_search\n",
    "parameters = {'criterion':('gini', 'entropy'), 'min_samples_split':[2,5,10,20],'min_samples_leaf':[1]}\n",
    "randomF = RandomForestClassifier(n_estimators=100,oob_score=True,n_jobs=-1,\n",
    "                                 max_features='auto')\n",
    "\n",
    "randomF.fit(X_train,Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7120"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(randomF.predict(X_test)==Y_test) #0.716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf1 = LogisticRegression(C=1.5,random_state=1)\n",
    "clf1.fit(X_train,Y_train)  #0.7846326"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensemble learning(majority vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77 (+/- 0.00) [Logistic Regression]\n",
      "Accuracy: 0.70 (+/- 0.00) [Random Forest]\n",
      "Accuracy: 0.75 (+/- 0.00) [SVM]\n",
      "Accuracy: 0.76 (+/- 0.00) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import cross_validation\n",
    "\n",
    "clf1 = LogisticRegression(C=1.5,random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators=100,oob_score=True,n_jobs=-1,\n",
    "                                 max_features='auto',random_state=1)\n",
    "clf3 = svm.SVC(C=1000)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('svm', clf3)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'SVM', 'Ensemble']):\n",
    "    scores = cross_validation.cross_val_score(clf, X_train, Y_train, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76 (+/- 0.00) [Logistic Regression]\n",
      "Accuracy: 0.68 (+/- 0.00) [Random Forest]\n",
      "Accuracy: 0.73 (+/- 0.00) [SVM]\n",
      "Accuracy: 0.76 (+/- 0.00) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import cross_validation\n",
    "\n",
    "clf1 = LogisticRegression(C=1.5,random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators=100,oob_score=True,n_jobs=-1,\n",
    "                                 max_features='auto',random_state=1)\n",
    "clf3 = svm.SVC(C=1000,probability=True)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('svm', clf3)], voting='soft')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'SVM', 'Ensemble']):\n",
    "    scores = cross_validation.cross_val_score(clf, X_train, Y_train, cv=2, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensemble learning(boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.001, loss='deviance',\n",
       "              max_depth=3, max_features=None, max_leaf_nodes=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=4000,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "gbc = ensemble.GradientBoostingClassifier(learning_rate=0.001,\n",
    "                                          max_depth=3,n_estimators=4000)\n",
    "gbc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6467"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(gbc.predict(X_test.toarray())==Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7542"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best so far\n",
    "sum(gbc.predict(X_test.toarray())==Y_test)#learning_rate=0.05, max_depth=3,n_estimators=1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fitting word2vec model into different estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm, grid_search\n",
    "\n",
    "parameters = {'weights':('uniform','distance'), 'n_neighbors':[7,9,11,13]}\n",
    "neigh = KNeighborsClassifier()\n",
    "neigh_word2vec = grid_search.GridSearchCV(neigh, parameters)\n",
    "neigh_word2vec.fit(X_word2vec_train,Y_word2vec_train) \n",
    "print neigh_word2vec.grid_scores_ , ' \\n ','Best Estimator',neigh_word2vec.best_estimator_\n",
    "# best knn\n",
    "best_knn= KNeighborsClassifier(n_neighbors=11,weights='distance')\n",
    "best_knn.fit(X_word2vec_train,Y_word2vec_train)\n",
    "sum(best_knn.predict(X_word2vec_test)==Y_word2vec_test)   #0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn import svm, grid_search\n",
    "parameters = {'C':[1e6,1e7],'kernal':('rbf','sigmoid')}\n",
    "svr_word2vec = svm.SVC()\n",
    "clf_word2vec = grid_search.GridSearchCV(svr_word2vec, parameters)\n",
    "clf_word2vec.fit(X_word2vec_train,Y_word2vec_train) \n",
    "print clf_word2vec.grid_scores_ , ' \\n ','Best Estimator',clf_word2vec.best_estimator_\n",
    "\n",
    "# best SVM\n",
    "best_knn= svm.SVC()(C=100000,kernal='rbf')\n",
    "best_knn.fit(X_word2vec_train,Y_word2vec_train)\n",
    "sum(best_knn.predict(X_word2vec_test)==Y_word2vec_test) #0.69779 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "from sklearn import linear_model, datasets,grid_search\n",
    "\n",
    "parameters = {'penalty':('l1', 'l2'), 'C':[1,10,100,1000]}\n",
    "lr = linear_model.LogisticRegression()\n",
    "clr = grid_search.GridSearchCV(lr, parameters)\n",
    "clr.fit(X_word2vec_train,Y_word2vec_train) \n",
    "print clr.grid_scores_ , ' \\n ','Best Estimator',clr.best_estimator_\n",
    "\n",
    "#best logistic Regression\n",
    "\n",
    "best_lr=LogisticRegression(C=100,penalty='l1')\n",
    "best_lr.fit(X_word2vec_train,Y_word2vec_train)\n",
    "sum(best_lr.predict(X_word2vec_test)==Y_word2vec_test) #0.68414"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Error:  0.0151\n",
      "Epoch: 1, Error:  0.0134\n",
      "Epoch: 2, Error:  0.0130\n"
     ]
    }
   ],
   "source": [
    "from pybrain.datasets import ClassificationDataSet\n",
    "from pybrain.tools.shortcuts import buildNetwork\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "from pybrain.structure.modules import SoftmaxLayer, TanhLayer,SigmoidLayer\n",
    "\n",
    "ds_train = ClassificationDataSet(X_word2vec_train.shape[1],1,nb_classes=20)\n",
    "\n",
    "for i in range(len(X_word2vec_train)):\n",
    "    ds_train.addSample(X_word2vec_train[i],Y_train[i])\n",
    "\n",
    "ds_train._convertToOneOfMany()\n",
    "fnn = buildNetwork(X_word2vec_train.shape[1], 160, 20, \n",
    "                   hiddenclass=SigmoidLayer,outclass=SoftmaxLayer)\n",
    "trainer = BackpropTrainer(fnn, ds_train)\n",
    "for i in range(3):\n",
    "    error = trainer.train()\n",
    "    print \"Epoch: %d, Error: %7.4f\" % (i, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6114"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "ypreds = []\n",
    "ytrues = []\n",
    "for i in range(X_word2vec_test.shape[0]):\n",
    "    pred = fnn.activate(X_word2vec_test[i, :])\n",
    "    ypreds.append(pred.argmax())\n",
    "    ytrues.append(Y_word2vec_test[i])\n",
    "sum(np.array(ypreds)==np.array(ytrues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Error:  0.0118\n",
      "Epoch: 1, Error:  0.0118\n",
      "Epoch: 2, Error:  0.0118\n",
      "Epoch: 3, Error:  0.0117\n",
      "Epoch: 4, Error:  0.0117\n",
      "Epoch: 5, Error:  0.0117\n",
      "Epoch: 6, Error:  0.0116\n",
      "Epoch: 7, Error:  0.0116\n",
      "Epoch: 8, Error:  0.0116\n",
      "Epoch: 9, Error:  0.0116\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    error = trainer.train()\n",
    "    print \"Epoch: %d, Error: %7.4f\" % (i, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network in cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1363L, 6714L)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.toarray()[np.where((Y_test==2) | (Y_test==4))].shape\n",
    "np.array(map(lambda x: clf1.coef_[x],Y_test[np.where((Y_test==cuisine1) | (Y_test==cuisine2))])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the cosine similarity matrix\n",
    "import numpy\n",
    "# choose two cusines to draw the network\n",
    "cuisine1=17\n",
    "cuisine2=4\n",
    "cuisine3=15\n",
    "\n",
    "\n",
    "# multiplying the bag of word models with the feature weight given by logistic regress\n",
    "dataframe=X_test.toarray()[np.where((Y_test==cuisine1) | (Y_test==cuisine2) | (Y_test==cuisine3))]*np.array(map(lambda x: clf1.coef_[x],Y_test[np.where((Y_test==cuisine1) | (Y_test==cuisine2)| (Y_test==cuisine3))]))\n",
    "\n",
    "# dataframe=X_test.toarray()[np.where((Y_test==cuisine1) | (Y_test==cuisine2) )]*np.array(map(lambda x: clf1.coef_[x],Y_test[np.where((Y_test==cuisine1) | (Y_test==cuisine2))]))\n",
    "\n",
    "#without multiplying the coefficient\n",
    "# dataframe=X_test.toarray()[np.where((Y_test==cuisine1) | (Y_test==cuisine2))]\n",
    "# dot production of the matrix\n",
    "similarity=dataframe.dot(dataframe.T)\n",
    "\n",
    "# find the inverse of the square root of the diagnoal elements\n",
    "square_mag = np.diag(similarity)\n",
    "inv_square_mag = 1.0 / square_mag\n",
    "inv_square_mag[numpy.isinf(inv_square_mag)] = 0\n",
    "inv_mag = numpy.sqrt(inv_square_mag)\n",
    "\n",
    "\n",
    "del dataframe, inv_square_mag\n",
    "# computing the cosine similarity\n",
    "cosine_similarity=similarity*inv_mag\n",
    "\n",
    "cosine_similarity=cosine_similarity.T*inv_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose a threshold value, below which we assume the restaurants are not connected\n",
    "\n",
    "# cosine_similarity[cosine_similarity<0.5]=0\n",
    "# cosine_similarity[np.where((cosine_similarity>0.5))]=1\n",
    "\n",
    "# generating the network\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import random\n",
    "\n",
    "\n",
    "g=nx.Graph(np.array([i[:] for i in cosine_similarity[:]]))\n",
    "# to calculate the connection between each restaurant with the spring model\n",
    "# however, this is not feasible as the size is too big\n",
    "\n",
    "# pos=nx.spring_layout(g,pos=None,dim=2,iterations=1000,scale=100,k=2)\n",
    "\n",
    "new_d={}\n",
    "for key,value in d.iteritems():\n",
    "    new_d[value]= key\n",
    "\n",
    "# assign different color to different cuisin\n",
    "new_cuisine=[new_d[i] for i in [cuisine1,cuisine2,cuisine3]]\n",
    "# new_cuisine=[new_d[i] for i in [cuisine1,cuisine2]]\n",
    "val_map = dict([(c, index) for index, c in enumerate(new_cuisine)])\n",
    "ColorLegend=dict([(c, index) for index, c in enumerate(new_cuisine)])\n",
    "values=[val_map[new_d[Y_test[np.where((Y_test==cuisine1) | (Y_test==cuisine2)| (Y_test==cuisine3))][node]]] for node in g.nodes()]\n",
    "# values=[val_map[new_d[Y_test[np.where((Y_test==cuisine1) | (Y_test==cuisine2))][node]]] for node in g.nodes()]\n",
    "\n",
    "\n",
    "# Color mapping\n",
    "jet =cm= plt.get_cmap('jet')\n",
    "cNorm  = colors.Normalize(vmin=0, vmax=max(values))\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=jet)\n",
    "\n",
    "# Using a figure to use it as a parameter when calling nx.draw_networkx\n",
    "f = plt.figure(1)\n",
    "ax = f.add_subplot(1,1,1)\n",
    "for label in ColorLegend:\n",
    "    ax.plot([0],[0],color=scalarMap.to_rgba(ColorLegend[label]),label=label)\n",
    "\n",
    "# Just fixed the color map\n",
    "nx.draw_networkx(g, cmap = jet, vmin=0, vmax= max(values),node_color=values,with_labels=False,ax=ax,\n",
    "                 arrows=False,node_size=30,alpha=0.5,width=0.0)\n",
    "\n",
    "# Setting it to how it was looking before.                                                                                                              \n",
    "# plt.axis('off')\n",
    "f.set_facecolor('w')\n",
    "\n",
    "plt.legend(loc='best',bbox_to_anchor=(1.3,1),borderpad=0.2)\n",
    "f.tight_layout()\n",
    "plt.show()\n",
    "# f.savefig(r'C:\\Users\\zhanlong\\Desktop\\cuisine.png',dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d={}\n",
    "for key,value in d.iteritems():\n",
    "    new_d[value]= key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cuisine=[new_d[i] for i in [cuisine1,cuisine2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'chinese'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_d[Y_test[np.where((Y_test==cuisine1) | (Y_test==cuisine2))][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'brazilian': 13,\n",
       " u'british': 9,\n",
       " u'cajun_creole': 12,\n",
       " u'chinese': 8,\n",
       " u'filipino': 2,\n",
       " u'french': 14,\n",
       " u'greek': 0,\n",
       " u'indian': 3,\n",
       " u'irish': 16,\n",
       " u'italian': 6,\n",
       " u'jamaican': 4,\n",
       " u'japanese': 15,\n",
       " u'korean': 17,\n",
       " u'mexican': 7,\n",
       " u'moroccan': 18,\n",
       " u'russian': 19,\n",
       " u'southern_us': 1,\n",
       " u'spanish': 5,\n",
       " u'thai': 10,\n",
       " u'vietnamese': 11}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
